{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping Project: scrapping top repositories of top topics on Github.\n",
    "\n",
    "- For this web scrapping project, the goal is to scrap Github's [Most popular Page](https://github.com/topics), and obtain the top repositories in each topic(in /data). \n",
    "- The site we are going to be scrapping is the topics page of github, it lists all most popular topics on github and provides a description and link to a topic page, which contains the most popular repositories for that topic. \n",
    "- Our goal is to obtain information about the top repositories in the most popular topics. \n",
    "- We will be using requests, Python, BeautifulSoup, Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The site we are going to be scrapping is 'https://github.com/topics'\n",
    "We will get the top 30 most popular topics on github, obtaining the topic titles, topic descriptions, and the link of the topics. \n",
    "Then, for each topic, we will scrap for the top 25 repositories under that topic. The information will be stored in csv files. For each repository, we will obtain the repository username, repository title, repository star reviews, and repository url. \n",
    "\n",
    "### Outline:\n",
    "- Scrap https://github.com/topics\n",
    "- obtain topic title, topic url, and topic description, and store in dataframe.\n",
    "- Using the topic url, obtain information about top respositories in each topic page.\n",
    "    - Parse the repository name, username, url, and star reviews. \n",
    "    - Save all topic repo data into csv files with format:\n",
    "    `repo_name,username,repo_url,stars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"webscrapping-prjt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parsing github topics main page\n",
    "Obtain topic title, topic description, and topic url, of the top 30 topics on the github topics page. Merge topic data into a dataframe. \n",
    "- store topic title, descrpition, and url in lists\n",
    "- combine lists into topic dictionary\n",
    "- transform topic dictionary into dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text doc into Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in topic page doc, and obtains a list of topic titles. \n",
    "def get_topic_title(doc):\n",
    "    # search for topic title tags in topic page doc. \n",
    "    selection_class = \"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "    topic_title_tag = doc.find_all('p', {'class': selection_class})\n",
    "    \n",
    "    # store parsed titles from the tag in list\n",
    "    topic_title = []\n",
    "    for tag in topic_title_tag:\n",
    "\n",
    "        topic_title.append(tag.text)\n",
    "\n",
    "    return topic_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_topic_title` obtains list of topic titles in main topic page, returns list of topic titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in topic page doc, and obtains a list of topic descriptions.  \n",
    "def get_topic_desc(doc):\n",
    "    # search for topic descriptions tags in topic page doc. \n",
    "    desc_select = \"f5 color-fg-muted mb-0 mt-1\"\n",
    "    topic_desc_tag = doc.find_all('p', {'class': desc_select})\n",
    "\n",
    "    descs = []\n",
    "    for desc in topic_desc_tag:\n",
    "        descs.append(desc.text.strip())\n",
    "    return descs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_topic_desc` obtains list of topic descriptions in main topic page, returns list of topic descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for topic url tags in topic page doc. \n",
    "def get_topic_url(doc):\n",
    "    # search for topic url tags in topic page doc. \n",
    "    topic_link_select = \"no-underline flex-1 d-flex flex-column\"\n",
    "    topic_link_tag = doc.find_all('a', {'class': topic_link_select})\n",
    "    \n",
    "    # combine base url with parsed topic url to create full url for topic link. \n",
    "    topic_url = []\n",
    "    base_url = \"https://github.com\"\n",
    "    \n",
    "    for url in topic_link_tag:\n",
    "        topic_url.append(base_url+url['href'])\n",
    "    return topic_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_topic_url` obtains path of the topic url, merges it with \"https://github.com\" and returns a list of functioning urls to topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function, run to return df of top 30 most popular topics in github. \n",
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    \n",
    "    # catch request exceptions\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to load topics page\")\n",
    "        \n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # using get_topic_title, get_topic_desc, get_topic_url to obtain lists of title, description, and url of topics. \n",
    "    topics_dict={'title': get_topic_title(doc), \n",
    "                \"description\": get_topic_desc(doc),\n",
    "                'url': get_topic_url(doc)}\n",
    "    \n",
    "    # transform dict into DF\n",
    "    return pd.DataFrame(topics_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrape_topics()` combines get_topic_url, get_topic_desc, get_topic_title. It combines the lists from the three funcs and stores them in a dictionary; returns topics data containing title, descriptions and url of all top 30 topics in a dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scrapping Info from Individual Topics pages\n",
    "Using the topics url from the main topics page data dataframe, go into each topics page and obtain data about the top 25 repos in each topic. \n",
    "\n",
    "In each topics page, \"h3\" represents a repo, and each repo has 2 \"a\" tags, the 1st: repo username, the 2nd: repo name. The 2nd \"a\" tag also contains a repo url we want to parse. There is also a star tag we want to parse that is not inside the \"h3\" tag. \n",
    "    \n",
    "Each star tag contains text information about how many stars the repo has obtained.\n",
    "\n",
    "- request individual topics page using topics url. \n",
    "- Obtain repo data from \"h3\" tags which contain repo username, repo name and url; and also repo star data from star tag. \n",
    "- Combine lists of repo data into dictionary, and into Dataframe.\n",
    "- Convert dataframe of topic repos into csv, and store in \"/data\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in topic url and path name, parses repo info from a topic, and stores repo data into csv file in /data. \n",
    "def scrap_topic(path, topic_url):\n",
    "    # skip file making proccess if file already exists. \n",
    "    if os.path.exists(path):\n",
    "        print(\"file already exists, Skipping...\")\n",
    "        return\n",
    "    # topic_df stores df of all repo info from single topic\n",
    "    topic_df = get_topic_repos(get_topics_page(topic_url))\n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrape_topics()` combines `get_topic_repos()`, `get_topics_page()`, storing repo info(initially from dataframe) into a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in topic url, returns html doc of topics page. \n",
    "def get_topics_page(topic_url):\n",
    "    \n",
    "    # obtain topic page using request\n",
    "    response = requests.get(topic_url)\n",
    "    \n",
    "    # catch request exceptions\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to load topics page\")\n",
    "        \n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_topics_page` takes in a topic page url, and returns topics page doc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a Topic page document, returns dataframe with repo data from the topic page. \n",
    "def get_topic_repos(topics_doc):\n",
    "    \n",
    "    # Select all repo_tags in topic_doc\n",
    "    repo_select = \"f3 color-fg-muted text-normal lh-condensed\"\n",
    "    repo_tags = topics_doc.find_all('h3', {'class': repo_select})\n",
    "    \n",
    "    # select all star_tags in topic_doc\n",
    "    star_select = \"Counter js-social-count\"\n",
    "    star_tags = topics_doc.find_all('span', {\"class\": star_select})\n",
    "    \n",
    "    # initialize dict to store repo data from topic page.  \n",
    "    topic_dict = {'repo_name':[],\n",
    "                 'username':[],\n",
    "                 'repo_url':[],\n",
    "                 'stars':[]}\n",
    "    \n",
    "    # loop through each repo tag(and star tag), and append each repo's data into a topic dictionary. \n",
    "    for n in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[n], star_tags[n])\n",
    "        topic_dict['repo_name'].append(repo_info[0])\n",
    "        topic_dict['username'].append(repo_info[1])\n",
    "        topic_dict['repo_url'].append(repo_info[2])\n",
    "        topic_dict['stars'].append(repo_info[3])\n",
    "    \n",
    "    # return dictionary in form of DF\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_topic_repos` takes in a topic page doc. Uses `get_repo_info()` to obtain repo data. Stores results from teh 2 funcs and returns dataframe with repo data(repo title, url, username and stars) from a single topic page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parses informaiton from ONE REPO: takes h3tag and star tag of repo, returns repo_name, username, repo_url, and stars. \n",
    "def get_repo_info(h3_tag, star_tag):\n",
    "    \n",
    "    # obtain repo_name and username info from a_tag\n",
    "    a_tag = h3_tag.find_all('a')\n",
    "    repo_name = a_tag[1].text.strip()\n",
    "    username = a_tag[0].text.strip()\n",
    "    \n",
    "    repo_url = \"http://github.com\" + a_tag[1]['href']\n",
    "    \n",
    "    # translates star data into numeric data\n",
    "    stars = parse_stars(star_tag.text.strip())\n",
    "    return repo_name, username, repo_url, stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_repo_info` obtains repo information(name, username, url, stars) from a repo tag. Uses `parse_stars()` to format star number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translates star data into numeric data. \n",
    "def parse_stars(stars):\n",
    "    if stars[-1] == 'k':\n",
    "        return int(float(stars[:-1])*1000)\n",
    "    else:\n",
    "        return int(stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parse_stas()` helper function that takes in a string containing star data, and returns the numeric value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mega Function: Putting it all together!\n",
    "- obtain and parse information from github main topics page. Store in Dataframe \"topic_df\"\n",
    "- Loop through each topic in the Dataframe, parse information about top repos in each topic, and store in csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def scrape_topics_repos():\n",
    "    path = Path().resolve()\n",
    "    \n",
    "    # Scrapping github main topics page. Storing data in topic_df. \n",
    "    topic_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs(str(path)+\"/data\", exist_ok=True)\n",
    "    \n",
    "    # Loop through each topic from topic_df, scrapping and storing repo info from each topic.\n",
    "    for index, row in topic_df.iterrows():\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        print(row['url'])\n",
    "        scrap_topic(str(path)+'/data/{}.csv'.format(row['title']), row['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top repositories for \"3D\"\n",
      "https://github.com/topics/3d\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Ajax\"\n",
      "https://github.com/topics/ajax\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Algorithm\"\n",
      "https://github.com/topics/algorithm\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Amp\"\n",
      "https://github.com/topics/amphp\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Android\"\n",
      "https://github.com/topics/android\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Angular\"\n",
      "https://github.com/topics/angular\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Ansible\"\n",
      "https://github.com/topics/ansible\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"API\"\n",
      "https://github.com/topics/api\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Arduino\"\n",
      "https://github.com/topics/arduino\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"ASP.NET\"\n",
      "https://github.com/topics/aspnet\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Atom\"\n",
      "https://github.com/topics/atom\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Awesome Lists\"\n",
      "https://github.com/topics/awesome\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Amazon Web Services\"\n",
      "https://github.com/topics/aws\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Azure\"\n",
      "https://github.com/topics/azure\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Babel\"\n",
      "https://github.com/topics/babel\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Bash\"\n",
      "https://github.com/topics/bash\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Bitcoin\"\n",
      "https://github.com/topics/bitcoin\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Bootstrap\"\n",
      "https://github.com/topics/bootstrap\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Bot\"\n",
      "https://github.com/topics/bot\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"C\"\n",
      "https://github.com/topics/c\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Chrome\"\n",
      "https://github.com/topics/chrome\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Chrome extension\"\n",
      "https://github.com/topics/chrome-extension\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Command line interface\"\n",
      "https://github.com/topics/cli\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Clojure\"\n",
      "https://github.com/topics/clojure\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Code quality\"\n",
      "https://github.com/topics/code-quality\n",
      "file already exists, Skipping...\n",
      "Scraping top repositories for \"Code review\"\n",
      "https://github.com/topics/code-review\n",
      "Scraping top repositories for \"Compiler\"\n",
      "https://github.com/topics/compiler\n",
      "Scraping top repositories for \"Continuous integration\"\n",
      "https://github.com/topics/continuous-integration\n",
      "Scraping top repositories for \"COVID-19\"\n",
      "https://github.com/topics/covid-19\n",
      "Scraping top repositories for \"C++\"\n",
      "https://github.com/topics/cpp\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference and summary\n",
    "- Summary: we parsed information on top topics from github's topics page, and stored info about 25-most-popular repositories from each topic in csv files. The csv files are in /data. \n",
    "\n",
    "- References: [Amazing tutorial that lead me through my first forray into web scrapping.](https://www.youtube.com/watch?v=RKsLLG-bzEY&t=6582s)\n",
    "- Ideas for future work: [books.toscrap](http://books.toscrape.com/catalogue/page-2.html), [Animal Crossing Villager popularity list](https://www.animalcrossingportal.com/games/new-horizons/guides/villager-popularity-list.php#/)(for a EDA project I wanted to work on in combination with [a Animal Crossing Dataset](https://www.kaggle.com/jessicali9530/animal-crossing-new-horizons-nookplaza-dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
